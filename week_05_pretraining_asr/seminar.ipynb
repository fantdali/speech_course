{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4667f501-a4b8-4cca-9d9f-bf1952f02679",
   "metadata": {},
   "source": [
    "# Pretraining for ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c28407e-2d86-4b6f-a5de-8f6660a24c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libs\n",
    "# !pip3 install torch torchvision torchaudio datasets transformers soundfile jiwer --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip3 install librosa --index-url https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23424d0d-66db-4a73-accb-4b5e1320a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import Wav2Vec2ForPreTraining, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa8ef5-4b17-4faa-a1a9-5435234f3e46",
   "metadata": {},
   "source": [
    "## Finetuning Wav2Vec2 model on CTC loss (5 points)\n",
    "\n",
    "\n",
    "In this task you have to create pipeline for finetuning pretrained multilingual Wav2Vec2 model on belarusian audio from [Fleurs](https://huggingface.co/datasets/google/fleurs) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c568-8567-497b-983c-c38c45642a9f",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8eeaa1-62f5-4ccc-8ace-74b023719835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fleurs = load_dataset(\"google/fleurs\", \"be_by\", split=[\"train\", \"validation\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b35708-f6a7-4d98-b26f-da8f70d87997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'вышыня двух пілонаў складае 83 метры даўжыня моста - 378 метраў праезная частка складаецца з дзвюх палос шырыня кожнай - 3,50 м'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleurs[0][\"transcription\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9aaf9-a7e8-460f-a51b-611f2bd7aaf2",
   "metadata": {},
   "source": [
    "In this task, you should:\n",
    "\n",
    "* filter all samples, where `transcription` includes digits. Hint: take care of specific belarussian symbols \"і\", \"ў\";\n",
    "* remove punctuation from `transcription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a755b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train, val, test = fleurs\n",
    "\n",
    "digit_re = re.compile(r\"\\d\")\n",
    "punct_table = str.maketrans(\"\", \"\", string.punctuation + \"«»—…“”‘’\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.translate(punct_table)\n",
    "    return text\n",
    "\n",
    "def filter_no_digits(batch):\n",
    "    return not digit_re.search(batch[\"transcription\"])\n",
    "\n",
    "def remove_punct(batch):\n",
    "    batch[\"transcription\"] = clean_text(batch[\"transcription\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7894b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "    num_rows: 967\n",
      "})\n",
      "before clean: дэль потра меў перавагу ў пачатку другога сэту аднак калі лік стаў 6-6 спатрэбіўся тай-брэйк\n",
      "after clean: дэль потра меў перавагу ў пачатку другога сэту аднак калі лік стаў 66 спатрэбіўся тайбрэйк\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "\n",
    "print(f\"before clean: {test[0][\"transcription\"]}\")\n",
    "print(f\"after clean: {clean_text(test[0][\"transcription\"])}\")\n",
    "print(filter_no_digits(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2df4169-4fce-48f0-881d-ff366ee20077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2433 1927\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = train.filter(filter_no_digits).map(remove_punct) # YOUR CODE HERE\n",
    "preprocessed_val = val.filter(filter_no_digits).map(remove_punct) # YOUR CODE HERE\n",
    "\n",
    "print(len(train), len(preprocessed_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dd623-c031-4066-b408-4337b67056e9",
   "metadata": {},
   "source": [
    "#### Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bdfb6-91fd-4b3f-b1f4-acb10f122a5b",
   "metadata": {},
   "source": [
    "There you should train your own BPE tokenizer based on texts from Fleurs dataset using [HuggingFace tokenizer](https://huggingface.co/docs/tokenizers/en/training_from_memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42affbc0-7b19-4fb4-ad36-faf51a211ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers, decoders\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "BOS_TOKEN = \"[BOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=UNK_TOKEN))\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "special_tokens = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "bpe_trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559c1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    (x[\"transcription\"] for x in preprocessed_train), trainer=bpe_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64071158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі\n",
      "['Ñĥ', 'ĠÑĤÐ¾Ð¹', 'ĠÐ¶', 'Ð°', 'ĠÑĩÐ°Ñģ', 'ĠÐ¿', 'Ð°Ð±Ð»Ñĸ', 'Ð·Ñĥ', 'ĠÐ°Ð´', 'ĠÐ²ÐµÑĢ', 'Ð°Ð³', 'Ð¾Ð´', 'Ð½ÑĭÑħ', 'ĠÐ¼Ð°ÑĢ', 'ÑĪ', 'ÑĢÑĥ', 'ÑĤÐ°Ñŀ', 'ĠÑĥ', 'Ð²Ð°ÑĢ', 'Ð²', 'Ð°Ð½Ð½Ñı', 'ĠÐ±', 'Ð°Ð·', 'ÑĸÑĢ', 'Ð°Ð²', 'Ð°Ð»Ð°ÑģÑı', 'ĠÐ²ÐµÐ»ÑĮÐ¼Ñĸ', 'ĠÐ¼', 'Ð°Ð»Ð°', 'ĠÐºÐ°ÑĢ', 'Ð°Ð±', 'Ð»', 'Ñĳ', 'Ñŀ', 'ĠÐºÐ°ÑĢ', 'Ð°Ð»Ðµ', 'Ñŀ', 'ÑģÐºÐ°Ð³Ð°', 'ĠÑĦ', 'Ð»Ð¾', 'ÑĤÑĥ', 'ĠÑĤÐ°Ð¼Ñĥ', 'ĠÑĪÑĤÐ¾', 'ĠÐ°Ð´', 'Ð¼Ñĸ', 'ÑĢ', 'Ð°Ð»', 'Ñĭ', 'ĠÐ°Ñģ', 'ÑĨ', 'ÐµÑĢ', 'Ð°Ð³', 'Ð°Ð»ÑĸÑģÑı', 'ĠÑĸÑħ', 'ĠÐ¿Ð°ÑĤ', 'Ð°Ð¿', 'Ð»ÐµÐ½Ð½Ñı', 'ĠÐ½Ñı', 'Ð¼Ðµ', 'ÑĨ', 'ÐºÑĸ', 'Ð¼Ñĸ', 'ĠÐ¿', 'Ð°Ð²Ðµ', 'ÑĤ', 'ÑĢÐ°Ð½', 'ÑĭÐ¼Ñĸ', 'ĠÑģÑĸ', 'Ð»', 'Ð°Ð¼Ñĸ']\n",
      "у той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_train[0][\"transcription\"])\n",
    "encoded = tokenizer.encode(preprocessed_train[0][\"transcription\"])\n",
    "print(encoded.tokens)\n",
    "print(tokenizer.decode(encoded.ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bee0f1-e988-477f-b608-56a5afba8f0d",
   "metadata": {},
   "source": [
    "#### Loading model and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9feb4b5b-8b94-4114-ab60-600bf50c69b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "   \"facebook/wav2vec2-xls-r-300m\"\n",
    ")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=tokenizer.token_to_id(PAD_TOKEN),\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d04550-d56a-4496-9a48-ea5164233fc5",
   "metadata": {},
   "source": [
    "#### Data processor and data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437cf4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'train/10009414287632395082.wav',\n",
       " 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00031281,\n",
       "        -0.00038069, -0.00132966]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b0be5db-079f-4589-9c21-80cdb9a94afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcDataProcessor:\n",
    "    def __init__(self, tokenizer, feature_extractor):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __call__(self, row):\n",
    "        \"\"\"\n",
    "            Function applies tokenizer on row['transcription'] and applies feature extractor on audio column in row.\n",
    "            Input: dict with transcription and audio fields\n",
    "            Output: original dict includes `labels` column with tokenized sequence and `input_values` column with computed spectrogram.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        features = self.feature_extractor(\n",
    "            row[\"audio\"][\"array\"], sampling_rate=row[\"audio\"][\"sampling_rate\"]\n",
    "        )\n",
    "        labels = self.tokenizer.encode(row[\"transcription\"]).ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": features[\"input_values\"][0],\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f42e7d4-e719-411b-bbfb-8eb9d4fb7ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d04f72daf2248718245b7a9853b411a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1927 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149d8fb293e432786aabd2ecbfe7955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_processor = CtcDataProcessor(tokenizer, feature_extractor)\n",
    "train = preprocessed_train.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_train.column_names)\n",
    "val = preprocessed_val.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d5e826f-2e25-41e0-be1f-330a2b14a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class CTCDataCollator:\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = []\n",
    "        max_label_length = max(len(l[\"input_ids\"]) for l in label_features)\n",
    "        \n",
    "        for label in label_features:\n",
    "            label_ids = label[\"input_ids\"]\n",
    "            remainder = [-100] * (max_label_length - len(label_ids))\n",
    "            labels_batch.append(label_ids + remainder)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(labels_batch, dtype=torch.long)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec64935-37c3-4947-a5d3-22842ca6f6ce",
   "metadata": {},
   "source": [
    "#### Inference and metrics computing\n",
    "\n",
    "There you should use simple greedy straregy for CTC output decoding. \n",
    "\n",
    "Hint: Don't forget about padding value -100 in reference.\n",
    "\n",
    "Hint: Don't forget about CTC output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "669c3a17-2256-4b56-8c2c-1cb0b6ae4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "class MetricsComputer:\n",
    "    def __call__(self, pred):\n",
    "        \"\"\"\n",
    "            Input: object with fields `predictions` for CTC model output and `label_ids` for tokenized reference;\n",
    "            Output: dict with key `wer` and computed wer\n",
    "        \"\"\"\n",
    "        preds_logits = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "        blank_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "        \n",
    "        pred_ids = np.argmax(preds_logits, axis=-1)\n",
    "        \n",
    "        batch_pred_ids = []\n",
    "        for seq in pred_ids:\n",
    "            filtered_seq = [token for token, _ in groupby(seq) if token != blank_id]\n",
    "            batch_pred_ids.append(filtered_seq)\n",
    "        \n",
    "        batch_label_ids = []\n",
    "        for seq in label_ids:\n",
    "            filtered_seq = [token for token in seq if token != -100]\n",
    "            batch_label_ids.append(filtered_seq)\n",
    "        \n",
    "        pred_str = [tokenizer.decode(ids) for ids in batch_pred_ids]\n",
    "        label_str = [tokenizer.decode(ids) for ids in batch_label_ids]\n",
    "    \n",
    "        print(f\"Prediction: '{pred_str[0]}'\")\n",
    "        print(f\"Reference: '{label_str[0]}'\")\n",
    "        \n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        print(f\"WER: {wer:.4f}\")\n",
    "        return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fd03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_metric.compute(predictions=[\"hello world\"], references=[\"hello word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa62c-b147-4fa7-9b47-77069b7e4fb3",
   "metadata": {},
   "source": [
    "#### Overfitting on train batch\n",
    "\n",
    "In this task you should check pipeline correctness by overfitting on you need to finetune Wav2Vec2 model and achieve 50 WER or lower accuracy on val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f00dd9bc-8449-4ae2-924c-660a2217cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    eval_strategy=\"steps\",\n",
    "    max_steps=3000,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-3,\n",
    "    warmup_steps=100,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    log_level=\"info\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ad6f56-50d2-4788-8d6c-c38a279a5597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainerCallback\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=CTCDataCollator(feature_extractor=feature_extractor, padding=True),\n",
    "    args=training_args,\n",
    "    compute_metrics=MetricsComputer(),\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362d922-6f53-4faf-820f-348674bb25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
